[//]: # (Image References)
[image1]: ./output/filters.png
[image2]: ./output/video_screenshot.png
[image3]: ./output/autonomous.png 

# Project Report: Search and Sample Return

## Introduction
Based on the NASA sample return challenge this project enables to experiment with the three core elements of robotics: perception, decision making and action. This report includes the explanation of some implementation choices following the [Rubric Points](https://review.udacity.com/#!/rubrics/916/view) required.

## Notebook Analysis
The provided Jupiter Notebook implements the same functions explained in the lectures. In this section, main customizatins are explained. The final Jupyter Notebooks implementation is availabe [here](code/Rover_Project_Test_Notebook.ipynb).

### 1. Run the functions provided in the notebook on test images (first with the test data provided, next on data you have recorded). Add/modify functions to allow for color selection of obstacles and rock samples.

The [recorded data](output/recording/) was captured using 1920x1080 resolution.

In order to process the images, the `color_thresh()` function was customized. Firstly, it was used an upper and lower limit for better results, especially for the yellow color. Secondly, it was used the function `inRange()`, from the OpenCV library to select the desired intervals. It provides a much more clean implementation than checking each color channel individually. 
   
Following the recomendation for navigable terrain and experimenting with RGB values for the yellow color, the following intervals were used to select the navigable terrain, the obstacles and the rock samples. 

```python
threshed_navigable = color_thresh(image, lower_thresh=(160, 160, 160), upper_thresh=(255, 255, 255))
threshed_obstacle = color_thresh(image, lower_thresh=(0, 0, 0), upper_thresh=(160, 160, 160))
threshed_rock = color_thresh(rock_img, lower_thresh=(100, 100, 0), upper_thresh=(255, 255, 50))
```

The results achieved using the intervals above can be seen in the image bellow.
![alt text][image1]

Possibly, better results could be achieved if the color-space was converted from RGB to HSV, in the latter, the intevals for specific color detection can be better defined. Considering that results using RGB values were satisfactory, optimizations will be done in the simulator to acieve better fidelity if necessary.

### 2. Populate the `process_image()` function with the appropriate analysis steps to map pixels identifying navigable terrain, obstacles and rock samples into a worldmap.  Run `process_image()` on your test data using the `moviepy` functions provided to create video output of your result. 

In summary, the `process_image()` performs the following steps:

1. Define points to create the perspective transform matrix

    This step, is not actually necessary within this function. The transform matrix do not need to be calculated every step.

2. Apply the perspective transform in the image

    The result represtes a top view that can be compared with the map. 
    The `perspective_mask` matrix with boolean values is also generated to represent the pixels that can actually be perceived by the camera.

3. Apply the color thresholds 

    This is used to identify the rock samples, obstacles and navigable terrain.

4. Convert each identified image for roover-centric coordinates. 

    The `perspective_mask` generated in the step 2 is used to keep only the relevant pixels.

5. Map the rover-centric pixels to world coords

    The `pix_to_world()` function is used to apply rotation and translation using the roover coords in the map.

6. Update worldmap image

    Add rocks samples, obstacles and navigable terrain.

7. Prepare a mosaic image

    The image is used in the [video](output/test_mapping.mp4) generation, which is generated by the last functions in the Jupyter Notebook.

The image bellow, caputered from the video, shows the results of the steps described.
![alt text][image2]


Several optimizations are possible at this point, such as refining color thresholds and discarding the readings when the rover is not stable. 
Since playing with the simulator is much more fun than with the Jupyter Notebook, some of this optimizations were only implemented in the second part of this project!


## Autonomous Navigation and Mapping

### 1. Fill in the `perception_step()` (at the bottom of the `perception.py` script) and `decision_step()` (in `decision.py`) functions in the autonomous mapping scripts and an explanation is provided in the writeup of how and why these functions were modified as they were.


#### `perception_step()` function considerations


The `perception_step()` function is very similar to the `process_image()` function described above. In order to improve fidelity results, some restrictions and improvements were made. 

First, when the rover was not stable, the reading was discarded. To do that, the Rover.worldmap was only updated when the pitch and roll were within aceptable limits. 
 
Additionally, locations that were far from the rover are more sensitve even for small movements. For this reason, pixels from the edge of the image were discarded, usine the `fov_mask` to crop the readings.

Finally, the last diference is that the `perception_step()` function updates variables that will be used by the decision phase, such as the `Rover.nav_dists` and `Rover.nav_angles`.


#### `decision_step()` function considerations

The `decision_step()` function relies on the data provided by the perception step. 

The steering angle is calculated as an average of the navigalbe angles. This aproach sometimes make the rover drive straight into a obstacle if the navigable terrain is simetrical on both sides. 

Several improvements are possible and are being made. Nonetheless current implementation can make the rover drive in autonomous mode.  

### 2. Launching in autonomous mode your rover can navigate and map autonomously.  Explain your results and how you might improve them in your writeup.  

#### Simulator Settings
|Setting   | Value       |
|----------|-------------|
|Resolution| 1920 x 1080 |
|Quality   | Fantastic   |
|FPS       | Aprox 10    |

#### Results
In most of the times, the rover can map more than 40% of the map with fidelity higher than 60%.

![alt text][image3]

#### Possible Improvements
1. Perception Improve color

- Refine color thresholds using HSV instead of RGB 
- Limit the FOV using polar coords

2. Decision

- Consider only close pixels in order to make navigation decision
- Keep track of visited terrain 
- Improve state machine to collect samples and get out from stuck situations.
- Aproach the rock samples efficiently

### Conclusions
Curent version of the implementation, already achieves the minimal results necessary. Further improvements will be done continuously within the time limit. 


